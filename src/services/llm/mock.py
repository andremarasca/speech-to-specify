"""Mock LLM provider for testing."""

import hashlib
import time

from src.lib.exceptions import LLMError


class MockProvider:
    """
    Mock LLM provider for testing purposes.

    Returns deterministic responses based on prompt content hash.
    Can be configured to simulate delays and failures.
    """

    def __init__(
        self,
        delay_ms: int = 0,
        fail_on_prompts: list[str] | None = None,
    ):
        """
        Initialize the mock provider.

        Args:
            delay_ms: Simulated latency in milliseconds (default: 0)
            fail_on_prompts: List of prompt substrings that should trigger failures
        """
        self._delay_ms = delay_ms
        self._fail_on_prompts = fail_on_prompts or []

    @property
    def provider_name(self) -> str:
        """Return the provider identifier."""
        return "mock"

    def complete(self, prompt: str) -> str:
        """
        Return a deterministic mock response.

        Args:
            prompt: The prompt to respond to

        Returns:
            str: A deterministic response based on prompt hash

        Raises:
            LLMError: If prompt contains a fail trigger or is empty
        """
        if not prompt or not prompt.strip():
            raise LLMError("Prompt cannot be empty", provider=self.provider_name)

        # Check for failure triggers
        for fail_trigger in self._fail_on_prompts:
            if fail_trigger in prompt:
                raise LLMError(
                    f"Simulated failure triggered by: {fail_trigger}", provider=self.provider_name
                )

        # Simulate latency
        if self._delay_ms > 0:
            time.sleep(self._delay_ms / 1000)

        # Generate deterministic response based on prompt hash
        prompt_hash = hashlib.md5(prompt.encode()).hexdigest()[:8]

        # Detect step from prompt content for realistic responses
        if "constitution" in prompt.lower() or "princípios" in prompt.lower():
            return self._mock_constitution(prompt_hash)
        elif "specification" in prompt.lower() or "especificação" in prompt.lower():
            return self._mock_specification(prompt_hash)
        elif "planning" in prompt.lower() or "planejamento" in prompt.lower():
            return self._mock_planning(prompt_hash)

        # Default response
        return f"""# Mock Response [{prompt_hash}]

## Generated Content

This is a mock response generated for testing purposes.

The prompt contained {len(prompt)} characters and produced hash `{prompt_hash}`.

## Structure

- Point 1: Mock content A
- Point 2: Mock content B
- Point 3: Mock content C

---
*Generated by MockProvider*
"""

    def _mock_constitution(self, hash_suffix: str) -> str:
        """Generate a mock constitution artifact."""
        return f"""# Constitution [{hash_suffix}]

## Core Principles

1. **Determinism**: All operations produce predictable, reproducible results.
2. **Auditability**: Every action is logged and traceable.
3. **Modularity**: Components are loosely coupled and independently testable.

## Behavioral Guidelines

- Always preserve partial results on failure
- Never hide errors or state changes
- Log all external interactions

## Constraints

- No magic behavior
- No implicit state
- No silent failures

---
*Generated by MockProvider*
"""

    def _mock_specification(self, hash_suffix: str) -> str:
        """Generate a mock specification artifact."""
        return f"""# Specification [{hash_suffix}]

## Feature Overview

Transform chaotic text into structured narrative artifacts.

## User Stories

### US1: Process Text
As a user, I want to provide chaotic text and receive structured artifacts.

### US2: Track Interactions
As an auditor, I want to see all LLM interactions logged.

## Requirements

- FR-001: Accept text input from file
- FR-002: Generate sequential artifacts
- FR-003: Persist all outputs

## Success Criteria

- [ ] Input produces 3 artifacts
- [ ] All LLM calls are logged
- [ ] Failures preserve partial work

---
*Generated by MockProvider*
"""

    def _mock_planning(self, hash_suffix: str) -> str:
        """Generate a mock planning artifact."""
        return f"""# Planning [{hash_suffix}]

## Implementation Strategy

### Phase 1: Foundation
- Set up project structure
- Implement core models
- Create persistence layer

### Phase 2: MVP
- Implement pipeline orchestrator
- Add LLM integration
- Create CLI entry point

### Phase 3: Polish
- Add error handling
- Improve logging
- Write documentation

## Task Breakdown

1. T001: Create project structure
2. T002: Implement models
3. T003: Add persistence
4. T004: Create orchestrator
5. T005: Add CLI

---
*Generated by MockProvider*
"""
